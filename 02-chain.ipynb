{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a193c8e3-51b3-4e05-9c36-549c4b595282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Your langchain RAG chain\n",
    "\n",
    "In this notebook, we'll put together our langchain application.\n",
    "\n",
    "It's best practice to build your chain as a separate file and reference it directly to avoid serialization issues. \n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=4008450564360146&notebook=%2F01-first-step%2Fchain&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F01-first-step%2Fchain&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2fec995-5671-4b16-b568-fc65d89a9e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1449352208797439>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvector_search\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorSearchClient\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks_langchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvectorstores\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DatabricksVectorSearch\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschema\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrunnable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RunnableLambda\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'databricks.vector_search'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'databricks.vector_search'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'databricks.vector_search'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-1449352208797439>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvector_search\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorSearchClient\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks_langchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvectorstores\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DatabricksVectorSearch\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschema\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrunnable\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RunnableLambda\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'databricks.vector_search'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks_langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import mlflow\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "model_config = mlflow.models.ModelConfig(development_config=\"rag_chain_config.yaml\")\n",
    "\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    endpoint=model_config.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=model_config.get(\"vector_search_index\"),\n",
    "    columns=[\"id\", \"file_name\", \"content\"],\n",
    ").as_retriever(search_kwargs={\"k\": 3}) # Number of search results that the retriever returns\n",
    "# Enable the RAG Studio Review App and MLFlow to properly display track and display retrieved chunks for evaluation\n",
    "mlflow.models.set_retriever_schema(primary_key=\"id\", text_column=\"content\", doc_uri=\"file_name\")\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt (keep only the text from chunks)\n",
    "def format_context(docs):\n",
    "    chunk_contents = [f\"Passage: {d.page_content}\\n\" for d in docs]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from databricks_langchain.chat_models import ChatDatabricks\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (  # System prompt contains the instructions\n",
    "            \"system\",\n",
    "            \"\"\"Você é um assistente que responde perguntas sobre o acervo histório de pessoas políticas no Brasil. Use os seguintes verbetes para responder à pergunta. Alguns pedaços de contexto podem ser irrelevantes, nesse caso você não deve usá-los para formar a resposta. Responda por padrão em português do Brasil.\n",
    "\n",
    "Context: {context}\"\"\",\n",
    "        ),\n",
    "        # User's question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 1500}\n",
    ")\n",
    "\n",
    "# Return the string contents of the most recent messages: [{...}] from the user to be used as input question\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"context\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(extract_user_query_string)\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "chain",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}